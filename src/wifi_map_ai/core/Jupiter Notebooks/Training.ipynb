{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2264baf-40e7-49af-a89d-e13c2f1909e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scikit-learn matplotlib lightgbm pandas seaborn catboost dill streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2a613e-8f96-487e-b8e7-8a735241d621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:33:47.726126400Z",
     "start_time": "2024-02-21T09:33:45.837576Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adimaria\\AppData\\Local\\Temp\\ipykernel_39220\\696866848.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor           # https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html\n",
    "from sklearn.ensemble import ExtraTreesRegressor               # https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html\n",
    "from sklearn.ensemble import RandomForestRegressor   \n",
    "#from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945dd352",
   "metadata": {},
   "source": [
    "Hyperparameters and RMSE function for estimators"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def rmse(y_test, y_pred):\n",
    "    return np.sqrt(np.average((y_test - y_pred) ** 2, axis=0))\n",
    "\n",
    "#def split_scale_data(X, Y, test_size=0.2, random_state=42):\n",
    "    #X = features.values\n",
    "\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale and transform the data since models (like SVR) can be sensitive to disparate data values\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    # Fit on the training data and transform test data based on this\n",
    "    # See https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data\n",
    "    #X_train = scaler.fit_transform(X_train)\n",
    "    #X_test = scaler.transform (X_test)\n",
    "\n",
    "    #return X_train, X_test, Y_train, Y\n",
    "\n",
    "params = {'extra_trees': {'n_estimators': 1000,\n",
    "                          'max_features': \"sqrt\",     # Out of 20000\n",
    "                          'max_depth': 5,\n",
    "                          'random_state': 42\n",
    "                          },\n",
    "            'xgboost': {'max_depth': 5,\n",
    "                        'n_estimators': 1000,\n",
    "                        'learning_rate': 0.03,\n",
    "                        'random_state': 42\n",
    "                        },\n",
    "            'lightgbm': {'task': 'train', \n",
    "                         'boosting': 'gbdt',\n",
    "                         'objective': 'regression',\n",
    "                         'metric': 'rmse',\n",
    "                         'num_boost_round':300,\n",
    "                         'learning_rate': 0.05,\n",
    "                         'max_depth': 15,\n",
    "                         'num_leaves': 32,\n",
    "                         'min_data_in_leaf': 200\n",
    "                         },\n",
    "            'catboost': {'learning_rate': 0.3, \n",
    "                      'depth': 6, \n",
    "                      'l2_leaf_reg': 3, \n",
    "                      'loss_function': 'MultiRMSE', \n",
    "                      'eval_metric': 'MultiRMSE', \n",
    "                      'task_type': 'CPU', \n",
    "                      'iterations': 150,\n",
    "                      'od_type': 'Iter', \n",
    "                      'boosting_type': 'Plain', \n",
    "                      'bootstrap_type': 'Bernoulli', \n",
    "                      'allow_const_label': True,\n",
    "                      },\n",
    "          'mlp': {'learning_rate': 'adaptive',\n",
    "                  'random_state': 42,\n",
    "                  'early_stopping': True,\n",
    "                  'max_iter': 500,\n",
    "                  'verbose': 1,\n",
    "                  'hidden_layer_sizes': (100, 50, 50)\n",
    "                 },\n",
    "          'gbsk':{},\n",
    "          'histgbsk': {},\n",
    "          'random_forest': {}\n",
    "}\n",
    "\n",
    "ESTIMATORS = {\n",
    "    'random_forest': RandomForestRegressor(**params[\"random_forest\"]),\n",
    "    'extra_trees': ExtraTreesRegressor(**params[\"extra_trees\"]),\n",
    "    #'xgboost': MultiOutputRegressor(XGBRegressor(**params[\"xgboost\"])),\n",
    "    # 'lightgbm': MultiOutputRegressor(LGBMRegressor(**params[\"lightgbm\"])),\n",
    "    'catboost': CatBoostRegressor(**params[\"catboost\"]),\n",
    "    'mlp': MLPRegressor(**params[\"mlp\"]),\n",
    "    'gbsk': MultiOutputRegressor(GradientBoostingRegressor(**params[\"gbsk\"])),\n",
    "    'histgbsk': MultiOutputRegressor(HistGradientBoostingRegressor(**params[\"histgbsk\"])),\n",
    "}"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:33:53.197728Z",
     "start_time": "2024-02-21T09:33:53.190356500Z"
    }
   },
   "id": "3ec07a9c-3c08-4ea9-922a-cd1a5f429fd5",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading files"
   ],
   "metadata": {},
   "id": "b1f170ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "main_path = Path().absolute()\n",
    "\n",
    "X = np.load(main_path / \"Data\" / \"Clean\" / \"X.npy\")\n",
    "Y = np.load(main_path / \"Data\" / \"Clean\" / \"Y.npy\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:34:00.075774200Z",
     "start_time": "2024-02-21T09:34:00.049928300Z"
    }
   },
   "id": "58c9ff83-f23e-44ce-9c30-dc4826b50022",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shape check"
   ],
   "metadata": {},
   "id": "9b45d7e4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input date is: (400, 2)\n",
      "The shape of the input date is: (400, 10201)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the input date is: {X.shape}\") # this should (400, 2)\n",
    "print(f\"The shape of the input date is: {Y.shape}\") # this should be (400, 10201)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:34:05.831910900Z",
     "start_time": "2024-02-21T09:34:05.815609500Z"
    }
   },
   "id": "7de74b61",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loop for testing multiple estimators at once"
   ],
   "metadata": {},
   "id": "b357c5ea"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with catboost\n",
      "0:\tlearn: 22.2219882\ttotal: 5.86s\tremaining: 14m 33s\n",
      "1:\tlearn: 20.8891676\ttotal: 11.3s\tremaining: 13m 53s\n",
      "2:\tlearn: 19.8811559\ttotal: 16.4s\tremaining: 13m 24s\n",
      "3:\tlearn: 19.2080982\ttotal: 21.6s\tremaining: 13m 8s\n",
      "4:\tlearn: 18.6959494\ttotal: 26.6s\tremaining: 12m 52s\n",
      "5:\tlearn: 18.2931266\ttotal: 31.9s\tremaining: 12m 44s\n",
      "6:\tlearn: 17.9654196\ttotal: 37.1s\tremaining: 12m 38s\n",
      "7:\tlearn: 17.7268626\ttotal: 42.2s\tremaining: 12m 29s\n",
      "8:\tlearn: 17.6087325\ttotal: 47.4s\tremaining: 12m 22s\n",
      "9:\tlearn: 17.3884549\ttotal: 52.8s\tremaining: 12m 18s\n",
      "10:\tlearn: 17.2463997\ttotal: 57.9s\tremaining: 12m 11s\n",
      "11:\tlearn: 17.1250769\ttotal: 1m 3s\tremaining: 12m 4s\n",
      "12:\tlearn: 16.9717893\ttotal: 1m 8s\tremaining: 11m 59s\n",
      "13:\tlearn: 16.7971680\ttotal: 1m 13s\tremaining: 11m 54s\n",
      "14:\tlearn: 16.6937506\ttotal: 1m 18s\tremaining: 11m 45s\n",
      "15:\tlearn: 16.5397389\ttotal: 1m 23s\tremaining: 11m 37s\n",
      "16:\tlearn: 16.4244702\ttotal: 1m 28s\tremaining: 11m 30s\n",
      "17:\tlearn: 16.2888457\ttotal: 1m 33s\tremaining: 11m 23s\n",
      "18:\tlearn: 16.1736540\ttotal: 1m 38s\tremaining: 11m 15s\n",
      "19:\tlearn: 16.0979417\ttotal: 1m 42s\tremaining: 11m 8s\n",
      "20:\tlearn: 16.0106014\ttotal: 1m 47s\tremaining: 11m 1s\n",
      "21:\tlearn: 15.9061293\ttotal: 1m 52s\tremaining: 10m 55s\n",
      "22:\tlearn: 15.8037892\ttotal: 1m 57s\tremaining: 10m 48s\n",
      "23:\tlearn: 15.6862446\ttotal: 2m 2s\tremaining: 10m 42s\n",
      "24:\tlearn: 15.5896811\ttotal: 2m 7s\tremaining: 10m 38s\n",
      "25:\tlearn: 15.5203724\ttotal: 2m 12s\tremaining: 10m 32s\n",
      "26:\tlearn: 15.4297444\ttotal: 2m 17s\tremaining: 10m 27s\n",
      "27:\tlearn: 15.3413480\ttotal: 2m 22s\tremaining: 10m 22s\n",
      "28:\tlearn: 15.2626979\ttotal: 2m 27s\tremaining: 10m 17s\n",
      "29:\tlearn: 15.1803980\ttotal: 2m 32s\tremaining: 10m 11s\n",
      "30:\tlearn: 15.1270465\ttotal: 2m 37s\tremaining: 10m 6s\n",
      "31:\tlearn: 15.0667789\ttotal: 2m 42s\tremaining: 10m\n",
      "32:\tlearn: 15.0149039\ttotal: 2m 48s\tremaining: 9m 55s\n",
      "33:\tlearn: 14.9066924\ttotal: 2m 53s\tremaining: 9m 50s\n",
      "34:\tlearn: 14.8395691\ttotal: 2m 58s\tremaining: 9m 45s\n",
      "35:\tlearn: 14.7675352\ttotal: 3m 3s\tremaining: 9m 39s\n",
      "36:\tlearn: 14.7189767\ttotal: 3m 8s\tremaining: 9m 34s\n",
      "37:\tlearn: 14.6835033\ttotal: 3m 13s\tremaining: 9m 29s\n",
      "38:\tlearn: 14.5980705\ttotal: 3m 18s\tremaining: 9m 24s\n",
      "39:\tlearn: 14.5551946\ttotal: 3m 23s\tremaining: 9m 18s\n",
      "40:\tlearn: 14.5152058\ttotal: 3m 28s\tremaining: 9m 13s\n",
      "41:\tlearn: 14.4339295\ttotal: 3m 33s\tremaining: 9m 8s\n",
      "42:\tlearn: 14.3317062\ttotal: 3m 38s\tremaining: 9m 3s\n",
      "43:\tlearn: 14.2637888\ttotal: 3m 43s\tremaining: 8m 57s\n",
      "44:\tlearn: 14.2174897\ttotal: 3m 48s\tremaining: 8m 52s\n",
      "45:\tlearn: 14.1627577\ttotal: 3m 53s\tremaining: 8m 47s\n",
      "46:\tlearn: 14.1019948\ttotal: 3m 58s\tremaining: 8m 42s\n",
      "47:\tlearn: 14.0422440\ttotal: 4m 3s\tremaining: 8m 37s\n",
      "48:\tlearn: 13.9838307\ttotal: 4m 8s\tremaining: 8m 32s\n",
      "49:\tlearn: 13.9013328\ttotal: 4m 13s\tremaining: 8m 26s\n",
      "50:\tlearn: 13.8451308\ttotal: 4m 18s\tremaining: 8m 21s\n",
      "51:\tlearn: 13.7983737\ttotal: 4m 23s\tremaining: 8m 16s\n",
      "52:\tlearn: 13.7380442\ttotal: 4m 28s\tremaining: 8m 11s\n",
      "53:\tlearn: 13.6637672\ttotal: 4m 33s\tremaining: 8m 6s\n",
      "54:\tlearn: 13.6006848\ttotal: 4m 38s\tremaining: 8m 1s\n",
      "55:\tlearn: 13.5503433\ttotal: 4m 43s\tremaining: 7m 56s\n",
      "56:\tlearn: 13.4805667\ttotal: 4m 48s\tremaining: 7m 51s\n",
      "57:\tlearn: 13.4195494\ttotal: 4m 53s\tremaining: 7m 46s\n",
      "58:\tlearn: 13.3776688\ttotal: 4m 59s\tremaining: 7m 41s\n",
      "59:\tlearn: 13.3386741\ttotal: 5m 4s\tremaining: 7m 36s\n",
      "60:\tlearn: 13.2965800\ttotal: 5m 9s\tremaining: 7m 31s\n",
      "61:\tlearn: 13.2428566\ttotal: 5m 14s\tremaining: 7m 26s\n",
      "62:\tlearn: 13.1569635\ttotal: 5m 19s\tremaining: 7m 21s\n",
      "63:\tlearn: 13.1114845\ttotal: 5m 24s\tremaining: 7m 15s\n",
      "64:\tlearn: 13.0675706\ttotal: 5m 29s\tremaining: 7m 10s\n",
      "65:\tlearn: 13.0304183\ttotal: 5m 34s\tremaining: 7m 5s\n",
      "66:\tlearn: 12.9892457\ttotal: 5m 39s\tremaining: 7m\n",
      "67:\tlearn: 12.9484377\ttotal: 5m 44s\tremaining: 6m 55s\n",
      "68:\tlearn: 12.8947748\ttotal: 5m 49s\tremaining: 6m 50s\n",
      "69:\tlearn: 12.8240140\ttotal: 5m 54s\tremaining: 6m 45s\n",
      "70:\tlearn: 12.7675640\ttotal: 5m 59s\tremaining: 6m 40s\n",
      "71:\tlearn: 12.7238795\ttotal: 6m 4s\tremaining: 6m 35s\n",
      "72:\tlearn: 12.6682541\ttotal: 6m 9s\tremaining: 6m 29s\n",
      "73:\tlearn: 12.6214997\ttotal: 6m 14s\tremaining: 6m 24s\n",
      "74:\tlearn: 12.5665666\ttotal: 6m 19s\tremaining: 6m 19s\n",
      "75:\tlearn: 12.5028676\ttotal: 6m 24s\tremaining: 6m 14s\n",
      "76:\tlearn: 12.4632511\ttotal: 6m 29s\tremaining: 6m 9s\n",
      "77:\tlearn: 12.4349766\ttotal: 6m 34s\tremaining: 6m 4s\n",
      "78:\tlearn: 12.3981984\ttotal: 6m 39s\tremaining: 5m 59s\n",
      "79:\tlearn: 12.3589684\ttotal: 6m 44s\tremaining: 5m 54s\n",
      "80:\tlearn: 12.3021293\ttotal: 6m 50s\tremaining: 5m 49s\n",
      "81:\tlearn: 12.2439012\ttotal: 6m 55s\tremaining: 5m 44s\n",
      "82:\tlearn: 12.2108281\ttotal: 7m\tremaining: 5m 39s\n",
      "83:\tlearn: 12.1688931\ttotal: 7m 5s\tremaining: 5m 34s\n",
      "84:\tlearn: 12.1286088\ttotal: 7m 10s\tremaining: 5m 29s\n",
      "85:\tlearn: 12.0797181\ttotal: 7m 15s\tremaining: 5m 23s\n",
      "86:\tlearn: 12.0258806\ttotal: 7m 20s\tremaining: 5m 18s\n",
      "87:\tlearn: 11.9712332\ttotal: 7m 25s\tremaining: 5m 13s\n",
      "88:\tlearn: 11.9240060\ttotal: 7m 30s\tremaining: 5m 8s\n",
      "89:\tlearn: 11.8771035\ttotal: 7m 35s\tremaining: 5m 3s\n",
      "90:\tlearn: 11.8331066\ttotal: 7m 40s\tremaining: 4m 58s\n",
      "91:\tlearn: 11.7990592\ttotal: 7m 45s\tremaining: 4m 53s\n",
      "92:\tlearn: 11.7614167\ttotal: 7m 50s\tremaining: 4m 48s\n",
      "93:\tlearn: 11.7045996\ttotal: 7m 55s\tremaining: 4m 43s\n",
      "94:\tlearn: 11.6579341\ttotal: 8m\tremaining: 4m 38s\n",
      "95:\tlearn: 11.6187228\ttotal: 8m 5s\tremaining: 4m 33s\n",
      "96:\tlearn: 11.5792264\ttotal: 8m 10s\tremaining: 4m 28s\n",
      "97:\tlearn: 11.5441688\ttotal: 8m 15s\tremaining: 4m 23s\n",
      "98:\tlearn: 11.5070916\ttotal: 8m 20s\tremaining: 4m 17s\n",
      "99:\tlearn: 11.4770463\ttotal: 8m 25s\tremaining: 4m 12s\n",
      "100:\tlearn: 11.4431463\ttotal: 8m 30s\tremaining: 4m 7s\n",
      "101:\tlearn: 11.4020329\ttotal: 8m 35s\tremaining: 4m 2s\n",
      "102:\tlearn: 11.3497462\ttotal: 8m 40s\tremaining: 3m 57s\n",
      "103:\tlearn: 11.3061324\ttotal: 8m 45s\tremaining: 3m 52s\n",
      "104:\tlearn: 11.2760350\ttotal: 8m 51s\tremaining: 3m 47s\n",
      "105:\tlearn: 11.2465631\ttotal: 8m 56s\tremaining: 3m 42s\n",
      "106:\tlearn: 11.2086465\ttotal: 9m 1s\tremaining: 3m 37s\n",
      "107:\tlearn: 11.1816327\ttotal: 9m 6s\tremaining: 3m 32s\n",
      "108:\tlearn: 11.1279592\ttotal: 9m 11s\tremaining: 3m 27s\n",
      "109:\tlearn: 11.1061074\ttotal: 9m 16s\tremaining: 3m 22s\n",
      "110:\tlearn: 11.0708830\ttotal: 9m 21s\tremaining: 3m 17s\n",
      "111:\tlearn: 11.0438035\ttotal: 9m 26s\tremaining: 3m 12s\n",
      "112:\tlearn: 11.0081484\ttotal: 9m 31s\tremaining: 3m 7s\n",
      "113:\tlearn: 10.9655403\ttotal: 9m 36s\tremaining: 3m 2s\n",
      "114:\tlearn: 10.9364868\ttotal: 9m 41s\tremaining: 2m 56s\n",
      "115:\tlearn: 10.9076979\ttotal: 9m 46s\tremaining: 2m 51s\n",
      "116:\tlearn: 10.8650356\ttotal: 9m 51s\tremaining: 2m 46s\n",
      "117:\tlearn: 10.8271936\ttotal: 9m 56s\tremaining: 2m 41s\n",
      "118:\tlearn: 10.7907220\ttotal: 10m 1s\tremaining: 2m 36s\n",
      "119:\tlearn: 10.7526035\ttotal: 10m 6s\tremaining: 2m 31s\n",
      "120:\tlearn: 10.7143369\ttotal: 10m 11s\tremaining: 2m 26s\n",
      "121:\tlearn: 10.6781549\ttotal: 10m 16s\tremaining: 2m 21s\n",
      "122:\tlearn: 10.6430757\ttotal: 10m 22s\tremaining: 2m 16s\n",
      "123:\tlearn: 10.6100881\ttotal: 10m 27s\tremaining: 2m 11s\n",
      "124:\tlearn: 10.5800539\ttotal: 10m 32s\tremaining: 2m 6s\n",
      "125:\tlearn: 10.5464963\ttotal: 10m 37s\tremaining: 2m 1s\n",
      "126:\tlearn: 10.5121119\ttotal: 10m 42s\tremaining: 1m 56s\n",
      "127:\tlearn: 10.4804032\ttotal: 10m 47s\tremaining: 1m 51s\n",
      "128:\tlearn: 10.4451544\ttotal: 10m 52s\tremaining: 1m 46s\n",
      "129:\tlearn: 10.4190740\ttotal: 10m 57s\tremaining: 1m 41s\n",
      "130:\tlearn: 10.3801279\ttotal: 11m 2s\tremaining: 1m 36s\n",
      "131:\tlearn: 10.3416764\ttotal: 11m 7s\tremaining: 1m 31s\n",
      "132:\tlearn: 10.3121777\ttotal: 11m 12s\tremaining: 1m 25s\n",
      "133:\tlearn: 10.2762931\ttotal: 11m 17s\tremaining: 1m 20s\n",
      "134:\tlearn: 10.2474730\ttotal: 11m 22s\tremaining: 1m 15s\n",
      "135:\tlearn: 10.2214215\ttotal: 11m 27s\tremaining: 1m 10s\n",
      "136:\tlearn: 10.1925542\ttotal: 11m 32s\tremaining: 1m 5s\n",
      "137:\tlearn: 10.1706302\ttotal: 11m 37s\tremaining: 1m\n",
      "138:\tlearn: 10.1386988\ttotal: 11m 42s\tremaining: 55.6s\n",
      "139:\tlearn: 10.1136005\ttotal: 11m 47s\tremaining: 50.5s\n",
      "140:\tlearn: 10.0860656\ttotal: 11m 52s\tremaining: 45.5s\n",
      "141:\tlearn: 10.0529909\ttotal: 11m 57s\tremaining: 40.4s\n",
      "142:\tlearn: 10.0177960\ttotal: 12m 2s\tremaining: 35.4s\n",
      "143:\tlearn: 9.9890071\ttotal: 12m 7s\tremaining: 30.3s\n",
      "144:\tlearn: 9.9556013\ttotal: 12m 12s\tremaining: 25.3s\n",
      "145:\tlearn: 9.9286478\ttotal: 12m 17s\tremaining: 20.2s\n",
      "146:\tlearn: 9.9027490\ttotal: 12m 22s\tremaining: 15.2s\n",
      "147:\tlearn: 9.8860793\ttotal: 12m 28s\tremaining: 10.1s\n",
      "148:\tlearn: 9.8614825\ttotal: 12m 33s\tremaining: 5.05s\n",
      "149:\tlearn: 9.8336335\ttotal: 12m 38s\tremaining: 0us\n",
      "The R2 score for catboost is: 0.4294328722639772\n",
      "Running with mlp\n",
      "Iteration 1, loss = 0.10851163\n",
      "Validation score: -2.780365\n",
      "Iteration 2, loss = 0.10186587\n",
      "Validation score: -2.467590\n",
      "Iteration 3, loss = 0.09319733\n",
      "Validation score: -2.067880\n",
      "Iteration 4, loss = 0.08241177\n",
      "Validation score: -1.598142\n",
      "Iteration 5, loss = 0.07006034\n",
      "Validation score: -1.099621\n",
      "Iteration 6, loss = 0.05753583\n",
      "Validation score: -0.655894\n",
      "Iteration 7, loss = 0.04738928\n",
      "Validation score: -0.401240\n",
      "Iteration 8, loss = 0.04283240\n",
      "Validation score: -0.411901\n",
      "Iteration 9, loss = 0.04464457\n",
      "Validation score: -0.482521\n",
      "Iteration 10, loss = 0.04589418\n",
      "Validation score: -0.412875\n",
      "Iteration 11, loss = 0.04331465\n",
      "Validation score: -0.287790\n",
      "Iteration 12, loss = 0.03966571\n",
      "Validation score: -0.207160\n",
      "Iteration 13, loss = 0.03754823\n",
      "Validation score: -0.190289\n",
      "Iteration 14, loss = 0.03687772\n",
      "Validation score: -0.200666\n",
      "Iteration 15, loss = 0.03686338\n",
      "Validation score: -0.204343\n",
      "Iteration 16, loss = 0.03666362\n",
      "Validation score: -0.188555\n",
      "Iteration 17, loss = 0.03593387\n",
      "Validation score: -0.158992\n",
      "Iteration 18, loss = 0.03490907\n",
      "Validation score: -0.136241\n",
      "Iteration 19, loss = 0.03393960\n",
      "Validation score: -0.126277\n",
      "Iteration 20, loss = 0.03338506\n",
      "Validation score: -0.121630\n",
      "Iteration 21, loss = 0.03296748\n",
      "Validation score: -0.105314\n",
      "Iteration 22, loss = 0.03236469\n",
      "Validation score: -0.076575\n",
      "Iteration 23, loss = 0.03161539\n",
      "Validation score: -0.049123\n",
      "Iteration 24, loss = 0.03097770\n",
      "Validation score: -0.030801\n",
      "Iteration 25, loss = 0.03050040\n",
      "Validation score: -0.016059\n",
      "Iteration 26, loss = 0.03003340\n",
      "Validation score: 0.001129\n",
      "Iteration 27, loss = 0.02948080\n",
      "Validation score: 0.018575\n",
      "Iteration 28, loss = 0.02892136\n",
      "Validation score: 0.032819\n",
      "Iteration 29, loss = 0.02837952\n",
      "Validation score: 0.046389\n",
      "Iteration 30, loss = 0.02783727\n",
      "Validation score: 0.061575\n",
      "Iteration 31, loss = 0.02729551\n",
      "Validation score: 0.076652\n",
      "Iteration 32, loss = 0.02673565\n",
      "Validation score: 0.091282\n",
      "Iteration 33, loss = 0.02617078\n",
      "Validation score: 0.105650\n",
      "Iteration 34, loss = 0.02561328\n",
      "Validation score: 0.120219\n",
      "Iteration 35, loss = 0.02514296\n",
      "Validation score: 0.135420\n",
      "Iteration 36, loss = 0.02468819\n",
      "Validation score: 0.150175\n",
      "Iteration 37, loss = 0.02421389\n",
      "Validation score: 0.163237\n",
      "Iteration 38, loss = 0.02384191\n",
      "Validation score: 0.176575\n",
      "Iteration 39, loss = 0.02343424\n",
      "Validation score: 0.191611\n",
      "Iteration 40, loss = 0.02306743\n",
      "Validation score: 0.203911\n",
      "Iteration 41, loss = 0.02275529\n",
      "Validation score: 0.213178\n",
      "Iteration 42, loss = 0.02248343\n",
      "Validation score: 0.222973\n",
      "Iteration 43, loss = 0.02226248\n",
      "Validation score: 0.231983\n",
      "Iteration 44, loss = 0.02207105\n",
      "Validation score: 0.240175\n",
      "Iteration 45, loss = 0.02192892\n",
      "Validation score: 0.246953\n",
      "Iteration 46, loss = 0.02181382\n",
      "Validation score: 0.251692\n",
      "Iteration 47, loss = 0.02172333\n",
      "Validation score: 0.254775\n",
      "Iteration 48, loss = 0.02166891\n",
      "Validation score: 0.258278\n",
      "Iteration 49, loss = 0.02159248\n",
      "Validation score: 0.261847\n",
      "Iteration 50, loss = 0.02156442\n",
      "Validation score: 0.264017\n",
      "Iteration 51, loss = 0.02149502\n",
      "Validation score: 0.263767\n",
      "Iteration 52, loss = 0.02146616\n",
      "Validation score: 0.265911\n",
      "Iteration 53, loss = 0.02141284\n",
      "Validation score: 0.268027\n",
      "Iteration 54, loss = 0.02137788\n",
      "Validation score: 0.269010\n",
      "Iteration 55, loss = 0.02133707\n",
      "Validation score: 0.270152\n",
      "Iteration 56, loss = 0.02130081\n",
      "Validation score: 0.270316\n",
      "Iteration 57, loss = 0.02127439\n",
      "Validation score: 0.270859\n",
      "Iteration 58, loss = 0.02124370\n",
      "Validation score: 0.271701\n",
      "Iteration 59, loss = 0.02121980\n",
      "Validation score: 0.272415\n",
      "Iteration 60, loss = 0.02118834\n",
      "Validation score: 0.272871\n",
      "Iteration 61, loss = 0.02116923\n",
      "Validation score: 0.273339\n",
      "Iteration 62, loss = 0.02115095\n",
      "Validation score: 0.274546\n",
      "Iteration 63, loss = 0.02113394\n",
      "Validation score: 0.275941\n",
      "Iteration 64, loss = 0.02112237\n",
      "Validation score: 0.276582\n",
      "Iteration 65, loss = 0.02110407\n",
      "Validation score: 0.276600\n",
      "Iteration 66, loss = 0.02106642\n",
      "Validation score: 0.274823\n",
      "Iteration 67, loss = 0.02107806\n",
      "Validation score: 0.276962\n",
      "Iteration 68, loss = 0.02105192\n",
      "Validation score: 0.278038\n",
      "Iteration 69, loss = 0.02102865\n",
      "Validation score: 0.278138\n",
      "Iteration 70, loss = 0.02101629\n",
      "Validation score: 0.276763\n",
      "Iteration 71, loss = 0.02100613\n",
      "Validation score: 0.281096\n",
      "Iteration 72, loss = 0.02097422\n",
      "Validation score: 0.282282\n",
      "Iteration 73, loss = 0.02093566\n",
      "Validation score: 0.280722\n",
      "Iteration 74, loss = 0.02096388\n",
      "Validation score: 0.283680\n",
      "Iteration 75, loss = 0.02093231\n",
      "Validation score: 0.285458\n",
      "Iteration 76, loss = 0.02091969\n",
      "Validation score: 0.286044\n",
      "Iteration 77, loss = 0.02086562\n",
      "Validation score: 0.285166\n",
      "Iteration 78, loss = 0.02086588\n",
      "Validation score: 0.287526\n",
      "Iteration 79, loss = 0.02082217\n",
      "Validation score: 0.288437\n",
      "Iteration 80, loss = 0.02081007\n",
      "Validation score: 0.288925\n",
      "Iteration 81, loss = 0.02078783\n",
      "Validation score: 0.289324\n",
      "Iteration 82, loss = 0.02077101\n",
      "Validation score: 0.290167\n",
      "Iteration 83, loss = 0.02074475\n",
      "Validation score: 0.292098\n",
      "Iteration 84, loss = 0.02072985\n",
      "Validation score: 0.292092\n",
      "Iteration 85, loss = 0.02071913\n",
      "Validation score: 0.291468\n",
      "Iteration 86, loss = 0.02068425\n",
      "Validation score: 0.293009\n",
      "Iteration 87, loss = 0.02066529\n",
      "Validation score: 0.294894\n",
      "Iteration 88, loss = 0.02064130\n",
      "Validation score: 0.296434\n",
      "Iteration 89, loss = 0.02062403\n",
      "Validation score: 0.298141\n",
      "Iteration 90, loss = 0.02059800\n",
      "Validation score: 0.298349\n",
      "Iteration 91, loss = 0.02057415\n",
      "Validation score: 0.297844\n",
      "Iteration 92, loss = 0.02055704\n",
      "Validation score: 0.298268\n",
      "Iteration 93, loss = 0.02053399\n",
      "Validation score: 0.300910\n",
      "Iteration 94, loss = 0.02050242\n",
      "Validation score: 0.302790\n",
      "Iteration 95, loss = 0.02049490\n",
      "Validation score: 0.302560\n",
      "Iteration 96, loss = 0.02045602\n",
      "Validation score: 0.301026\n",
      "Iteration 97, loss = 0.02044280\n",
      "Validation score: 0.302971\n",
      "Iteration 98, loss = 0.02042336\n",
      "Validation score: 0.303491\n",
      "Iteration 99, loss = 0.02038199\n",
      "Validation score: 0.302323\n",
      "Iteration 100, loss = 0.02037805\n",
      "Validation score: 0.304975\n",
      "Iteration 101, loss = 0.02035387\n",
      "Validation score: 0.307216\n",
      "Iteration 102, loss = 0.02031611\n",
      "Validation score: 0.306657\n",
      "Iteration 103, loss = 0.02029413\n",
      "Validation score: 0.307264\n",
      "Iteration 104, loss = 0.02027543\n",
      "Validation score: 0.310396\n",
      "Iteration 105, loss = 0.02023474\n",
      "Validation score: 0.311810\n",
      "Iteration 106, loss = 0.02020778\n",
      "Validation score: 0.313019\n",
      "Iteration 107, loss = 0.02018355\n",
      "Validation score: 0.314183\n",
      "Iteration 108, loss = 0.02015328\n",
      "Validation score: 0.314117\n",
      "Iteration 109, loss = 0.02013523\n",
      "Validation score: 0.315514\n",
      "Iteration 110, loss = 0.02013002\n",
      "Validation score: 0.317647\n",
      "Iteration 111, loss = 0.02007869\n",
      "Validation score: 0.316690\n",
      "Iteration 112, loss = 0.02006403\n",
      "Validation score: 0.318807\n",
      "Iteration 113, loss = 0.02002826\n",
      "Validation score: 0.320193\n",
      "Iteration 114, loss = 0.02001060\n",
      "Validation score: 0.321375\n",
      "Iteration 115, loss = 0.01999825\n",
      "Validation score: 0.322333\n",
      "Iteration 116, loss = 0.01997525\n",
      "Validation score: 0.324157\n",
      "Iteration 117, loss = 0.01992659\n",
      "Validation score: 0.322729\n",
      "Iteration 118, loss = 0.01992290\n",
      "Validation score: 0.325538\n",
      "Iteration 119, loss = 0.01987875\n",
      "Validation score: 0.326832\n",
      "Iteration 120, loss = 0.01987962\n",
      "Validation score: 0.327950\n",
      "Iteration 121, loss = 0.01983637\n",
      "Validation score: 0.328741\n",
      "Iteration 122, loss = 0.01980880\n",
      "Validation score: 0.330434\n",
      "Iteration 123, loss = 0.01978196\n",
      "Validation score: 0.329350\n",
      "Iteration 124, loss = 0.01976062\n",
      "Validation score: 0.329832\n",
      "Iteration 125, loss = 0.01973449\n",
      "Validation score: 0.332147\n",
      "Iteration 126, loss = 0.01971893\n",
      "Validation score: 0.333681\n",
      "Iteration 127, loss = 0.01966859\n",
      "Validation score: 0.332256\n",
      "Iteration 128, loss = 0.01967112\n",
      "Validation score: 0.336191\n",
      "Iteration 129, loss = 0.01961977\n",
      "Validation score: 0.337447\n",
      "Iteration 130, loss = 0.01960342\n",
      "Validation score: 0.337639\n",
      "Iteration 131, loss = 0.01956972\n",
      "Validation score: 0.339074\n",
      "Iteration 132, loss = 0.01954586\n",
      "Validation score: 0.339164\n",
      "Iteration 133, loss = 0.01951407\n",
      "Validation score: 0.338861\n",
      "Iteration 134, loss = 0.01949394\n",
      "Validation score: 0.340840\n",
      "Iteration 135, loss = 0.01946677\n",
      "Validation score: 0.342533\n",
      "Iteration 136, loss = 0.01943624\n",
      "Validation score: 0.341397\n",
      "Iteration 137, loss = 0.01942610\n",
      "Validation score: 0.342645\n",
      "Iteration 138, loss = 0.01936844\n",
      "Validation score: 0.345109\n",
      "Iteration 139, loss = 0.01937123\n",
      "Validation score: 0.344377\n",
      "Iteration 140, loss = 0.01931840\n",
      "Validation score: 0.343313\n",
      "Iteration 141, loss = 0.01930536\n",
      "Validation score: 0.348014\n",
      "Iteration 142, loss = 0.01928798\n",
      "Validation score: 0.348293\n",
      "Iteration 143, loss = 0.01925264\n",
      "Validation score: 0.346176\n",
      "Iteration 144, loss = 0.01925681\n",
      "Validation score: 0.350600\n",
      "Iteration 145, loss = 0.01917619\n",
      "Validation score: 0.349543\n",
      "Iteration 146, loss = 0.01914500\n",
      "Validation score: 0.351318\n",
      "Iteration 147, loss = 0.01912136\n",
      "Validation score: 0.352374\n",
      "Iteration 148, loss = 0.01910474\n",
      "Validation score: 0.352040\n",
      "Iteration 149, loss = 0.01906776\n",
      "Validation score: 0.355067\n",
      "Iteration 150, loss = 0.01903949\n",
      "Validation score: 0.356709\n",
      "Iteration 151, loss = 0.01902477\n",
      "Validation score: 0.357211\n",
      "Iteration 152, loss = 0.01900935\n",
      "Validation score: 0.358561\n",
      "Iteration 153, loss = 0.01896656\n",
      "Validation score: 0.359541\n",
      "Iteration 154, loss = 0.01893376\n",
      "Validation score: 0.360862\n",
      "Iteration 155, loss = 0.01891401\n",
      "Validation score: 0.364597\n",
      "Iteration 156, loss = 0.01889519\n",
      "Validation score: 0.366633\n",
      "Iteration 157, loss = 0.01890303\n",
      "Validation score: 0.367100\n",
      "Iteration 158, loss = 0.01885379\n",
      "Validation score: 0.367410\n",
      "Iteration 159, loss = 0.01882862\n",
      "Validation score: 0.365159\n",
      "Iteration 160, loss = 0.01880912\n",
      "Validation score: 0.369056\n",
      "Iteration 161, loss = 0.01876293\n",
      "Validation score: 0.370381\n",
      "Iteration 162, loss = 0.01875546\n",
      "Validation score: 0.369631\n",
      "Iteration 163, loss = 0.01875494\n",
      "Validation score: 0.370940\n",
      "Iteration 164, loss = 0.01871932\n",
      "Validation score: 0.371525\n",
      "Iteration 165, loss = 0.01869273\n",
      "Validation score: 0.370279\n",
      "Iteration 166, loss = 0.01869161\n",
      "Validation score: 0.374086\n",
      "Iteration 167, loss = 0.01863099\n",
      "Validation score: 0.374787\n",
      "Iteration 168, loss = 0.01864353\n",
      "Validation score: 0.375130\n",
      "Iteration 169, loss = 0.01860169\n",
      "Validation score: 0.376337\n",
      "Iteration 170, loss = 0.01859362\n",
      "Validation score: 0.377692\n",
      "Iteration 171, loss = 0.01855786\n",
      "Validation score: 0.376726\n",
      "Iteration 172, loss = 0.01855608\n",
      "Validation score: 0.377656\n",
      "Iteration 173, loss = 0.01852265\n",
      "Validation score: 0.378312\n",
      "Iteration 174, loss = 0.01851466\n",
      "Validation score: 0.378071\n",
      "Iteration 175, loss = 0.01849695\n",
      "Validation score: 0.379006\n",
      "Iteration 176, loss = 0.01846964\n",
      "Validation score: 0.379108\n",
      "Iteration 177, loss = 0.01848760\n",
      "Validation score: 0.380725\n",
      "Iteration 178, loss = 0.01846282\n",
      "Validation score: 0.380723\n",
      "Iteration 179, loss = 0.01844865\n",
      "Validation score: 0.382396\n",
      "Iteration 180, loss = 0.01841096\n",
      "Validation score: 0.380766\n",
      "Iteration 181, loss = 0.01843979\n",
      "Validation score: 0.382696\n",
      "Iteration 182, loss = 0.01839677\n",
      "Validation score: 0.383091\n",
      "Iteration 183, loss = 0.01839285\n",
      "Validation score: 0.383164\n",
      "Iteration 184, loss = 0.01837277\n",
      "Validation score: 0.384559\n",
      "Iteration 185, loss = 0.01835294\n",
      "Validation score: 0.383067\n",
      "Iteration 186, loss = 0.01834320\n",
      "Validation score: 0.381438\n",
      "Iteration 187, loss = 0.01832598\n",
      "Validation score: 0.383906\n",
      "Iteration 188, loss = 0.01828410\n",
      "Validation score: 0.384989\n",
      "Iteration 189, loss = 0.01827452\n",
      "Validation score: 0.385203\n",
      "Iteration 190, loss = 0.01826931\n",
      "Validation score: 0.386009\n",
      "Iteration 191, loss = 0.01823026\n",
      "Validation score: 0.384034\n",
      "Iteration 192, loss = 0.01826658\n",
      "Validation score: 0.385816\n",
      "Iteration 193, loss = 0.01821140\n",
      "Validation score: 0.386083\n",
      "Iteration 194, loss = 0.01819205\n",
      "Validation score: 0.386484\n",
      "Iteration 195, loss = 0.01821477\n",
      "Validation score: 0.387077\n",
      "Iteration 196, loss = 0.01816760\n",
      "Validation score: 0.386794\n",
      "Iteration 197, loss = 0.01818086\n",
      "Validation score: 0.388572\n",
      "Iteration 198, loss = 0.01816421\n",
      "Validation score: 0.389421\n",
      "Iteration 199, loss = 0.01812167\n",
      "Validation score: 0.388671\n",
      "Iteration 200, loss = 0.01812140\n",
      "Validation score: 0.390405\n",
      "Iteration 201, loss = 0.01811099\n",
      "Validation score: 0.391651\n",
      "Iteration 202, loss = 0.01807956\n",
      "Validation score: 0.390868\n",
      "Iteration 203, loss = 0.01810145\n",
      "Validation score: 0.392686\n",
      "Iteration 204, loss = 0.01808292\n",
      "Validation score: 0.392693\n",
      "Iteration 205, loss = 0.01808413\n",
      "Validation score: 0.391735\n",
      "Iteration 206, loss = 0.01806645\n",
      "Validation score: 0.390956\n",
      "Iteration 207, loss = 0.01803883\n",
      "Validation score: 0.390460\n",
      "Iteration 208, loss = 0.01804032\n",
      "Validation score: 0.393855\n",
      "Iteration 209, loss = 0.01799707\n",
      "Validation score: 0.394657\n",
      "Iteration 210, loss = 0.01800323\n",
      "Validation score: 0.394477\n",
      "Iteration 211, loss = 0.01799808\n",
      "Validation score: 0.394253\n",
      "Iteration 212, loss = 0.01795750\n",
      "Validation score: 0.392915\n",
      "Iteration 213, loss = 0.01796349\n",
      "Validation score: 0.392780\n",
      "Iteration 214, loss = 0.01797318\n",
      "Validation score: 0.394467\n",
      "Iteration 215, loss = 0.01796800\n",
      "Validation score: 0.394253\n",
      "Iteration 216, loss = 0.01792340\n",
      "Validation score: 0.393927\n",
      "Iteration 217, loss = 0.01795370\n",
      "Validation score: 0.395573\n",
      "Iteration 218, loss = 0.01792141\n",
      "Validation score: 0.393035\n",
      "Iteration 219, loss = 0.01791116\n",
      "Validation score: 0.392136\n",
      "Iteration 220, loss = 0.01795234\n",
      "Validation score: 0.395992\n",
      "Iteration 221, loss = 0.01789560\n",
      "Validation score: 0.393481\n",
      "Iteration 222, loss = 0.01790594\n",
      "Validation score: 0.392456\n",
      "Iteration 223, loss = 0.01790616\n",
      "Validation score: 0.395974\n",
      "Iteration 224, loss = 0.01784048\n",
      "Validation score: 0.394969\n",
      "Iteration 225, loss = 0.01785830\n",
      "Validation score: 0.396285\n",
      "Iteration 226, loss = 0.01781370\n",
      "Validation score: 0.396882\n",
      "Iteration 227, loss = 0.01781160\n",
      "Validation score: 0.397712\n",
      "Iteration 228, loss = 0.01778279\n",
      "Validation score: 0.397229\n",
      "Iteration 229, loss = 0.01779887\n",
      "Validation score: 0.399565\n",
      "Iteration 230, loss = 0.01778010\n",
      "Validation score: 0.398886\n",
      "Iteration 231, loss = 0.01775737\n",
      "Validation score: 0.397124\n",
      "Iteration 232, loss = 0.01775461\n",
      "Validation score: 0.398525\n",
      "Iteration 233, loss = 0.01773242\n",
      "Validation score: 0.400222\n",
      "Iteration 234, loss = 0.01771844\n",
      "Validation score: 0.399893\n",
      "Iteration 235, loss = 0.01773205\n",
      "Validation score: 0.399725\n",
      "Iteration 236, loss = 0.01769900\n",
      "Validation score: 0.398320\n",
      "Iteration 237, loss = 0.01769853\n",
      "Validation score: 0.399195\n",
      "Iteration 238, loss = 0.01770508\n",
      "Validation score: 0.400575\n",
      "Iteration 239, loss = 0.01767974\n",
      "Validation score: 0.402132\n",
      "Iteration 240, loss = 0.01767723\n",
      "Validation score: 0.401602\n",
      "Iteration 241, loss = 0.01765380\n",
      "Validation score: 0.398759\n",
      "Iteration 242, loss = 0.01766523\n",
      "Validation score: 0.400628\n",
      "Iteration 243, loss = 0.01766733\n",
      "Validation score: 0.401137\n",
      "Iteration 244, loss = 0.01763254\n",
      "Validation score: 0.399602\n",
      "Iteration 245, loss = 0.01764853\n",
      "Validation score: 0.400534\n",
      "Iteration 246, loss = 0.01766493\n",
      "Validation score: 0.399749\n",
      "Iteration 247, loss = 0.01763459\n",
      "Validation score: 0.400982\n",
      "Iteration 248, loss = 0.01760513\n",
      "Validation score: 0.401176\n",
      "Iteration 249, loss = 0.01765234\n",
      "Validation score: 0.403754\n",
      "Iteration 250, loss = 0.01758544\n",
      "Validation score: 0.400509\n",
      "Iteration 251, loss = 0.01759810\n",
      "Validation score: 0.399586\n",
      "Iteration 252, loss = 0.01762913\n",
      "Validation score: 0.402829\n",
      "Iteration 253, loss = 0.01756469\n",
      "Validation score: 0.400404\n",
      "Iteration 254, loss = 0.01758215\n",
      "Validation score: 0.401963\n",
      "Iteration 255, loss = 0.01756167\n",
      "Validation score: 0.403240\n",
      "Iteration 256, loss = 0.01752220\n",
      "Validation score: 0.403209\n",
      "Iteration 257, loss = 0.01755359\n",
      "Validation score: 0.403527\n",
      "Iteration 258, loss = 0.01752889\n",
      "Validation score: 0.401112\n",
      "Iteration 259, loss = 0.01751557\n",
      "Validation score: 0.400597\n",
      "Iteration 260, loss = 0.01752401\n",
      "Validation score: 0.404533\n",
      "Iteration 261, loss = 0.01750181\n",
      "Validation score: 0.405135\n",
      "Iteration 262, loss = 0.01749892\n",
      "Validation score: 0.404700\n",
      "Iteration 263, loss = 0.01746769\n",
      "Validation score: 0.403621\n",
      "Iteration 264, loss = 0.01747689\n",
      "Validation score: 0.405075\n",
      "Iteration 265, loss = 0.01745096\n",
      "Validation score: 0.407904\n",
      "Iteration 266, loss = 0.01745187\n",
      "Validation score: 0.407476\n",
      "Iteration 267, loss = 0.01744752\n",
      "Validation score: 0.405387\n",
      "Iteration 268, loss = 0.01744099\n",
      "Validation score: 0.403417\n",
      "Iteration 269, loss = 0.01744249\n",
      "Validation score: 0.405066\n",
      "Iteration 270, loss = 0.01741993\n",
      "Validation score: 0.407113\n",
      "Iteration 271, loss = 0.01744358\n",
      "Validation score: 0.408035\n",
      "Iteration 272, loss = 0.01742646\n",
      "Validation score: 0.406559\n",
      "Iteration 273, loss = 0.01742014\n",
      "Validation score: 0.404962\n",
      "Iteration 274, loss = 0.01741804\n",
      "Validation score: 0.407273\n",
      "Iteration 275, loss = 0.01741141\n",
      "Validation score: 0.408166\n",
      "Iteration 276, loss = 0.01739142\n",
      "Validation score: 0.405879\n",
      "Iteration 277, loss = 0.01741849\n",
      "Validation score: 0.406304\n",
      "Iteration 278, loss = 0.01737762\n",
      "Validation score: 0.408597\n",
      "Iteration 279, loss = 0.01735878\n",
      "Validation score: 0.409359\n",
      "Iteration 280, loss = 0.01736656\n",
      "Validation score: 0.409493\n",
      "Iteration 281, loss = 0.01735158\n",
      "Validation score: 0.408853\n",
      "Iteration 282, loss = 0.01734354\n",
      "Validation score: 0.408243\n",
      "Iteration 283, loss = 0.01734625\n",
      "Validation score: 0.409289\n",
      "Iteration 284, loss = 0.01732738\n",
      "Validation score: 0.409956\n",
      "Iteration 285, loss = 0.01732800\n",
      "Validation score: 0.409255\n",
      "Iteration 286, loss = 0.01732226\n",
      "Validation score: 0.408845\n",
      "Iteration 287, loss = 0.01732688\n",
      "Validation score: 0.409617\n",
      "Iteration 288, loss = 0.01731356\n",
      "Validation score: 0.409720\n",
      "Iteration 289, loss = 0.01730002\n",
      "Validation score: 0.410814\n",
      "Iteration 290, loss = 0.01730654\n",
      "Validation score: 0.410400\n",
      "Iteration 291, loss = 0.01728465\n",
      "Validation score: 0.408247\n",
      "Iteration 292, loss = 0.01729848\n",
      "Validation score: 0.409545\n",
      "Iteration 293, loss = 0.01728818\n",
      "Validation score: 0.410864\n",
      "Iteration 294, loss = 0.01726805\n",
      "Validation score: 0.410492\n",
      "Iteration 295, loss = 0.01729285\n",
      "Validation score: 0.410649\n",
      "Iteration 296, loss = 0.01728758\n",
      "Validation score: 0.408049\n",
      "Iteration 297, loss = 0.01727819\n",
      "Validation score: 0.407766\n",
      "Iteration 298, loss = 0.01723860\n",
      "Validation score: 0.409804\n",
      "Iteration 299, loss = 0.01728710\n",
      "Validation score: 0.411189\n",
      "Iteration 300, loss = 0.01727658\n",
      "Validation score: 0.408587\n",
      "Iteration 301, loss = 0.01724224\n",
      "Validation score: 0.405251\n",
      "Iteration 302, loss = 0.01727260\n",
      "Validation score: 0.409546\n",
      "Iteration 303, loss = 0.01727465\n",
      "Validation score: 0.412304\n",
      "Iteration 304, loss = 0.01723439\n",
      "Validation score: 0.409152\n",
      "Iteration 305, loss = 0.01726227\n",
      "Validation score: 0.407628\n",
      "Iteration 306, loss = 0.01724185\n",
      "Validation score: 0.410352\n",
      "Iteration 307, loss = 0.01721564\n",
      "Validation score: 0.409917\n",
      "Iteration 308, loss = 0.01721229\n",
      "Validation score: 0.410783\n",
      "Iteration 309, loss = 0.01722266\n",
      "Validation score: 0.412278\n",
      "Iteration 310, loss = 0.01724326\n",
      "Validation score: 0.412887\n",
      "Iteration 311, loss = 0.01721404\n",
      "Validation score: 0.410922\n",
      "Iteration 312, loss = 0.01720113\n",
      "Validation score: 0.410573\n",
      "Iteration 313, loss = 0.01722250\n",
      "Validation score: 0.411501\n",
      "Iteration 314, loss = 0.01717701\n",
      "Validation score: 0.412355\n",
      "Iteration 315, loss = 0.01718229\n",
      "Validation score: 0.414450\n",
      "Iteration 316, loss = 0.01716872\n",
      "Validation score: 0.413822\n",
      "Iteration 317, loss = 0.01715695\n",
      "Validation score: 0.411029\n",
      "Iteration 318, loss = 0.01715613\n",
      "Validation score: 0.410406\n",
      "Iteration 319, loss = 0.01715043\n",
      "Validation score: 0.412333\n",
      "Iteration 320, loss = 0.01713470\n",
      "Validation score: 0.413767\n",
      "Iteration 321, loss = 0.01714627\n",
      "Validation score: 0.414367\n",
      "Iteration 322, loss = 0.01712658\n",
      "Validation score: 0.415595\n",
      "Iteration 323, loss = 0.01711984\n",
      "Validation score: 0.414719\n",
      "Iteration 324, loss = 0.01712974\n",
      "Validation score: 0.415496\n",
      "Iteration 325, loss = 0.01711511\n",
      "Validation score: 0.415905\n",
      "Iteration 326, loss = 0.01711409\n",
      "Validation score: 0.415043\n",
      "Iteration 327, loss = 0.01710658\n",
      "Validation score: 0.413032\n",
      "Iteration 328, loss = 0.01710215\n",
      "Validation score: 0.414099\n",
      "Iteration 329, loss = 0.01709157\n",
      "Validation score: 0.413674\n",
      "Iteration 330, loss = 0.01710186\n",
      "Validation score: 0.414568\n",
      "Iteration 331, loss = 0.01708919\n",
      "Validation score: 0.415090\n",
      "Iteration 332, loss = 0.01708270\n",
      "Validation score: 0.414420\n",
      "Iteration 333, loss = 0.01711618\n",
      "Validation score: 0.415063\n",
      "Iteration 334, loss = 0.01707956\n",
      "Validation score: 0.413628\n",
      "Iteration 335, loss = 0.01709524\n",
      "Validation score: 0.413217\n",
      "Iteration 336, loss = 0.01708629\n",
      "Validation score: 0.414102\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The R2 score for mlp is: 0.3749765900945988\n",
      "Running with gbsk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning with \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m%\u001B[39mname)\n\u001B[0;32m      3\u001B[0m X_train, X_test, Y_train, Y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, Y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m                    \u001B[38;5;66;03m# fit() with instantiated object\u001B[39;00m\n\u001B[0;32m      5\u001B[0m Y_predicted \u001B[38;5;241m=\u001B[39m estimator\u001B[38;5;241m.\u001B[39mpredict(X_test)   \u001B[38;5;66;03m# Make predictions and save it in dict under key: name\u001B[39;00m\n\u001B[0;32m      6\u001B[0m Y_rmse \u001B[38;5;241m=\u001B[39m rmse(Y_test, Y_predicted)\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\multioutput.py:272\u001B[0m, in \u001B[0;36m_MultiOutputEstimator.fit\u001B[1;34m(self, X, y, sample_weight, **fit_params)\u001B[0m\n\u001B[0;32m    269\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    270\u001B[0m         routed_params\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mfit[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample_weight\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m sample_weight\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_ \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_features_in_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mn_features_in_\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\joblib\\parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    127\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\multioutput.py:61\u001B[0m, in \u001B[0;36m_fit_estimator\u001B[1;34m(estimator, X, y, sample_weight, **fit_params)\u001B[0m\n\u001B[0;32m     59\u001B[0m     estimator\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 61\u001B[0m     estimator\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m estimator\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:784\u001B[0m, in \u001B[0;36mBaseGradientBoosting.fit\u001B[1;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resize_state()\n\u001B[0;32m    783\u001B[0m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[1;32m--> 784\u001B[0m n_stages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[0;32m    798\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_stages \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:880\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stages\u001B[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[0m\n\u001B[0;32m    873\u001B[0m         initial_loss \u001B[38;5;241m=\u001B[39m factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss(\n\u001B[0;32m    874\u001B[0m             y_true\u001B[38;5;241m=\u001B[39my_oob_masked,\n\u001B[0;32m    875\u001B[0m             raw_prediction\u001B[38;5;241m=\u001B[39mraw_predictions[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[0;32m    876\u001B[0m             sample_weight\u001B[38;5;241m=\u001B[39msample_weight_oob_masked,\n\u001B[0;32m    877\u001B[0m         )\n\u001B[0;32m    879\u001B[0m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[1;32m--> 880\u001B[0m raw_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    883\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    886\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[38;5;66;03m# track loss\u001B[39;00m\n\u001B[0;32m    893\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:490\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stage\u001B[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[0m\n\u001B[0;32m    487\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;241m*\u001B[39m sample_mask\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[0;32m    489\u001B[0m X \u001B[38;5;241m=\u001B[39m X_csc \u001B[38;5;28;01mif\u001B[39;00m X_csc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[1;32m--> 490\u001B[0m \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_g_view\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m    492\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[0;32m    495\u001B[0m X_for_tree_update \u001B[38;5;241m=\u001B[39m X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\tree\\_classes.py:1377\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m   1347\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1349\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m   1350\u001B[0m \n\u001B[0;32m   1351\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1374\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1377\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1378\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1379\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1380\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1382\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\AnsysDev\\venvs\\wifi_signal_distribution_mapping-venv\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    462\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    463\u001B[0m         splitter,\n\u001B[0;32m    464\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    470\u001B[0m     )\n\u001B[1;32m--> 472\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for name, estimator in ESTIMATORS.items():\n",
    "   print(\"Running with %s\"%name)\n",
    "   X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "   estimator.fit(X_train, Y_train)                    # fit() with instantiated object\n",
    "   Y_predicted = estimator.predict(X_test)   # Make predictions and save it in dict under key: name\n",
    "   Y_rmse = rmse(Y_test, Y_predicted)\n",
    "   print(f\"The R2 score for {name} is: {estimator.score(X_test, Y_test)}\")"
   ],
   "metadata": {},
   "id": "c83cf1b3-ab6f-4347-bb06-5e2e787721e6",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing "
   ],
   "metadata": {},
   "id": "4194cb7d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c927f2-aa8d-42e0-9df2-3b582fc34893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T10:25:43.406233700Z",
     "start_time": "2024-02-21T10:06:22.366963800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for the GradientBoostingRegressor is: 0.4293969612845138\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "# estimator = MultiOutputRegressor(GradientBoostingRegressor(**params[\"gbsk\"]))\n",
    "estimator = MultiOutputRegressor(RandomForestRegressor(**params[\"random_forest\"]))\n",
    "estimator.fit(X_train, Y_train)                    # fit() with instantiated object\n",
    "Y_predicted = estimator.predict(X_test)   # Make predictions and save it in dict under key: name\n",
    "Y_rmse = rmse(Y_test, Y_predicted)\n",
    "print(f\"The R2 score for the GradientBoostingRegressor is: {estimator.score(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "# estimator = MultiOutputRegressor(GradientBoostingRegressor(**params[\"gbsk\"]))\n",
    "estimator = RandomForestRegressor(**params[\"random_forest\"])\n",
    "estimator.fit(X_train, Y_train)                    # fit() with instantiated object\n",
    "Y_predicted = estimator.predict(X_test)   # Make predictions and save it in dict under key: name\n",
    "Y_rmse = rmse(Y_test, Y_predicted)\n",
    "print(f\"The R2 score for the GradientBoostingRegressor is: {estimator.score(X_test, Y_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T10:33:18.075431100Z"
    }
   },
   "id": "3d01a3e664e726b9"
  },
  {
   "cell_type": "markdown",
   "id": "ac60a498",
   "metadata": {},
   "source": [
    "Finding new energy based on position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9cc94-b93f-4cfe-a56a-667f7ba4941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_position = 1.5, 0.5\n",
    "new_Energy = estimator.predict(new_position)\n",
    "np.savetxt(\"new_E.csv\", new_Energy, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038427a",
   "metadata": {},
   "source": [
    "Writing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed04d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(estimator, open(main_path / 'model.dill', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f7bc7",
   "metadata": {},
   "source": [
    "Importing dilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_path):\n",
    "    print(f\"Loading model\")\n",
    "    mlmodel_path = model_path / \"model.dill\"\n",
    "    with open(mlmodel_path, 'rb') as f:\n",
    "        model = dill.load(f)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
